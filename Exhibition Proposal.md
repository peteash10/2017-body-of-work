# Instructions for Humans

*Exhibition proposal for BOM (Birmingham Open Media). February 2017.*

## Preamble

A lot of my work has been about interrogating closed systems or devices which the user cannot access the workings of - also known as [black box systems](https://en.wikipedia.org/wiki/Black_box) - attempting to reverse engineer them by putting the "wrong" things in and seeing what comes out. (See, for example, [Sitting In Stagram](http://art.peteashton.com/sitting-in-stagram/), exploring Instagram's compression algorithms by re-posting the same image over and over.) By looking at these unintended results I can start to understand how the intended results are created and reclaim some power over the system. 

In this way I build a relationship with the system similar in ways to how one might build a relationship with another person by talking to them and asking them questions which are outside the expected context in order to understand the functions of their mind. The difference between people and closed systems is there's a feedback loop with people. They are learning about you as you are learning about them. Your series of questions, and your response to the answers, will affect how they subsequently answer, for example.

Machine Learning systems are black box systems. They produce valuable information that is vital to the function of contemporary society but do so in a way that society does not understand. Even the people who programme them don't fully understand what's going on inside their massively complex processes. They also "learn", using successes and failures along with new information to generate more accurate and tailored results. With this level of complexity it is possible to imagine something analogous to a person. An artificial intelligence. 

But Machine Learning systems are not people. They are merely complex mathematical functions generating statistics about the world. It is a mistake to see them as building towards some kind of intelligence, let alone consciousness. 

However, because the varying outputs of ML systems *look* like intelligence, we *project* intelligence on to them, despite them being mathematical abstractions of representations of the world. The machine is not drawing a face or writing a poem. There is no intention or desire behind the machine's actions. When we see intelligence we are making it up in our minds. That's interesting. 

(This also applies, in some ways, to the humanisation and personification of animals. I do it all the time with my rabbits. Which is why I'll never understand my rabbits.)

If Art is the practice of creating abstractions of the world in order to better comprehend the world, the Machine Learning systems are ripe for artistic investigation. 

## Instructions for Humans

**Instructions for Humans** is new work by Pete Ashton developed during 2017 for exhibition at [Birmingham Open Media][5] in November 2017.

The works aim to explore how machine gesture informs the human creative process and in turn how human gesture might inform mechanical representations. By employing recent developments in artificial intelligence and machine learning I will be asking what it means for a computer to "see", how society can be influenced by opinions derived from the perceptions of machines, and how interrogating mechanical systems can help us to question the biases of our own sense-based cognition.

I will be producing the work during the upheavals of Brexit and Trump, upsets said to be informed by algorithmic distribution of "fake news" and data-mining by [companies like Cambridge Analytica](https://www.theguardian.com/politics/2017/feb/26/robert-mercer-breitbart-war-on-media-steve-bannon-donald-trump-nigel-farage). In this climate the work will address how machine learning affects concepts of filters, truth, objectivity and the fungibility of facts, looking into how we might develop tactics to understand and deal with this media environment. 

The work will balance serious themes with an explorative and educational approach, encouraging audiences to think about these processes and systems in modern society and question the place of cameras and other sensors in an era of massive data processing by governments and corporations. My end goal is to develop work which reveals the man behind the curtain but also dispels the confused fear and despair that often inform discussion of these issues.  

The work will be developed through the summer and culminate as an evolving performative exhibition directed by Machine Learning systems, which are in turn directed by the artist. 

Machine Learning systems, commonly called "algorithms" and "Artificial Intelligence", are complex statistical programmes which use vast quantities of data to predict a likely outcome. A simple example is Predictive Text or Autocorrect which notices the phrases you commonly type into your phone and suggests them to you. A more controversial case is predictive policing which uses historical crime data to suggest where police resources would best be deployed. In all cases, the algorithm is "trained" on a corpus of data and all its results are constrained by the quantity and quality of this information. 

A significant part of Machine Learning in recent years has involved image analysis and computer vision, for use in areas such as the development of self-driving cars and next-generation surveillance. This has caught the attention of both visual artists and privacy campaigners and will form the basis for my work. 

## The Work

Instructions for Humans will comprise digital, sculptural, and performance artworks generated by Machine Learning programmes. The work will be centred in a gallery and comprise the following parts. 

**The Black Box** - A sealed box containing the computer running the Machine Learning system which receives information about the city from a variety of sources, processes it into "models" which are employed to generate instructions and parameters for the artworks. This represents the proprietary, closed systems such as Google and Facebook which can only be partially understood through interrogating them. 

Accompanying this will be an educational resource, "Inside The Black Box", which will explain the workings of Machine Learning systems and their social implications to the layperson. 

**Portrait of the City** - a digital visual/sculptural artwork involving screens, projections and digitally fabricated objects which forms the centrepiece and backdrop of the exhibition. Within broad outlines set by the artist, the work will be continuously generated from data produced by The Black Box throughout the duration of the show. The work will comprise:

- Visualisations of information about the city on screens showing useful and absurd  conclusions. 
- Images generated from the corpus of information in the system, attempting to show how the machine sees the city.
- Objects fabricated to specifications determined by the output of the system using laser cutting and 3D printing. This collection of objects will grow during the exhibition with iterations in design showing the changes in the information corpus. 
- Kinetic sculptures which move according to the data sent to them by the system, creating a machine-gesture performance informed by the city. 

The arrangement of the objects and screens will be determined by the system which will produce instructions for gallery staff each day. 

**Instructions for Humans** - A series of performance and public participatory works will be scheduled during the exhibition, where directions are generated by the system immediately prior to the event. There will also be instructions available for gallery visitors to take away as well as published online. 

Performance artists ([Emily Warner][9] and [Aleks Wojtulewicz][10] are on board) will use generated outputs from the system as a "score" for movement. These outputs will range from explicit instructions to interpreting shapes, sounds and colours. We shall workshop this during the summer and performances will take place in the gallery and around the city during the exhibition. Documentation will be exhibited in the gallery and fed back into the system. 

A number of guided tours will take place during the exhibition where participants will be invited to perceive their city as a machine. Routes and instructions for how to think and behave will be algorithmically generated and issued to participants. Any materials gathered during these tours, such a photographs or written observations, will be fed back into the system. 

The system will generate instructions to be disseminated via social media, such as Twitter bots asking for photos of specific objects or surveys about the city. Instructions will also be available on the website. 

The work will culminate in a publication titled *Instructions for Humans* containing the instructions along with guidance on using them to inspire creative practice. 

## Events

There will be a series of public events prior to and during the exhibition itself, culminating in an online resource and the exhibition will be informed by the concerns, ideas and practices emerging from those events. 

**Workshops on using Machine Learning in the Arts** using simple implementations that can run on laptop computers or in the browser. These will be based on or inspired by:

- [Gene Kogan's Machine Learning for Artists](http://ml4a.github.io) book and online course.
- [Rebecca Fiebrink's Wekinator](http://www.wekinator.org) 
- [Rebecca Fiebrink's Machine Learning for Musicians and Artists](https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info) online course.
 

**A symposium for Machine Learning in the Arts**, in collaboration with [Luba Elliott][17] who runs the [CreativeAI](https://www.meetup.com/Creative-AI/) events in London. I would hope to gather local, national and international experts and practitioners, in person and via Skype, with the aim of inspiring future activity in the West Midlands and develop a community to support my practice. 

## Practicalities

Running on a custom-built Linux computer, my systems will be trained on information coming from a variety of sensors placed around the city along with data gathered from internet queries.

These sensors (with permission of property owners) will capture video, still images, environmental information, sound, radio signals and other data. Official data sources for weather, traffic, crime, pollution etc will also be captured. Online queries of social media and news sites will concentrate on issues deemed pertinent to the city such as homelessness, immigration and economic development, and will collect text and images.

People will be invited to upload their own photographs and data relating to the city and during the exhibition there will be an opportunity to do this within the gallery and on the website. All documentation material gathered during the project will also be submitted to the system for processing. 

The work will be developed by myself in collaboration with others as appropriate. I will use the [Fizzpop][3] hackspace in Digbeth for most of my fabrication and hope to collaborate with some of the members there. I will develop and preview visual work through the [Black Hole Club][4] at Vivid Projects. Substantial pieces of the work will be developed through residencies at [Birmingham Open Media][5]. I also run a low-key monthly artists feedback group at [P Cafe][6] in Stirchley. 

During the production period I will be in communication with other artists and scientists working in this field as well as attending the [Resonate][7] festival in Serbia (and other related events across Europe as funds permit) to network, develop and promote the work. 

All work will be documented online with the intention of creating a valuable resource for creation of similar work by others. 

## Inspirational works

**In the field of Machine Learning**

- Ross Godwin's [Word Camera](http://word.camera)
- Whatever [Memo Atken is working on](http://www.memo.tv/research/)
- [Quasimondo's work](http://mario-klingemann.tumblr.com) with [pix2pix][19]
- [Hanah Davis](http://www.hannahishere.com) - generating music from prose.
- Gene Kogan's [Machine Learning for Artists][18] book and videos. 
 
**Sensor Data and Experimental Capture**

- [Di Wiltshire](http://www.bom.org.uk/bom-fellows/di-wiltshire/)'s performance using biodata.
- [Tess Osbourne](http://www.birmingham.ac.uk/schools/gees/people/dr-students/osborne-tess.aspx) - Technogeographer at the University of Birmingham
- [Students of Golan Levin's Experimental Capture course.](https://github.com/golanlevin/ExperimentalCapture/tree/master/students)

### Timeline

|  Date  |  Activity |
| ------ | ------ |
| March | Submit G4A |
|| Learn Machine Learning
| April | Resonate Festival |
|  | Hear from ACE |
| May | Start Work |
|| Build Sensors |
| June | Build Robot AI |
|| Test sensors in City |
|| Residency / Workshop at BOM on sensors? (Exp Capt 2) |
| July | Translate AI Outputs |
| Aug | Work with Performers (Residency) |
| Sept | Produce prototype sculptures for exhibition |
| Oct | End Development process |
|| Tie it all together |
| Nov | Exhibition opens |
|| Walks, performances, talks, etc |
|| AI WM Symposium |	
| Jan | Documentation online |
|| Evaluation for ACE |
|| Raise profile |
|| Explore touring to other galleries / cities. |
|| Submit talks to conferences. |


## Residency Proposals

### Human / Machine Gesture Research

A short residency to work with performance artists to develop ways to interpret the outputs of a machine learning system as instructions for physical works.

This will involve researching the use of "scores" in dance and movement and learning a lot about the role of gesture in performance art, something I am currently quite ignorant of. 

Residency requirements would involve co-working space for coding and hardware development and Triangle Studio for practical work. 

### Experimental Capture II

During June I will be developing and testing sensors to be placed around the city during the exhibition. A month long residency will enable me to work with Fellows and use a central location. 

This can involve workshop(s) following on from the [Art & Tech Social I ran in January 2017](http://www.bom.org.uk/event/artandtech-pete-ashton/).  

### ML-WM symposium

**A symposium for Machine Learning in the Arts and Society**, in collaboration with Luba Elliott (Creative AI events in London), Peter Lewis from Aston University's [ALICE](https://alice.aston.ac.uk) and the Birmingham Open Rights Group. I plan to gather local, national and international experts and practitioners, in person and via Skype, with the aim of inspiring future activity in the West Midlands.

Taking place during **Instructions For Humans** exhibition at Birmingham Open Media. 

Produced by [Jenny Duffin](http://jennyduffin.com).  
Based at [Birmingham Open Media](http://bom.org.uk), a centre for exploring the intersection of art and science. 

**Schedule**

- Daytime: Workshops based around Gene Kogan's [ML4A](http://ml4a.github.io) and Rebecca Fiebrink's [Wekinator](http://wekinator.org).  
- Afternoon / Evening: Talks / panel discussions. 

**Speakers to include** 

- An artist working with machine learning (eg [Memo Akten](http://www.memo.tv/))
- An academic applying machine learning to civic problems (eg [Peter Lewis](http://prlewis.com)) 
- An expert on sociopolitical implications of machine learning (eg [Kate Crawford](http://www.katecrawford.net))

<hr />

[19]: https://github.com/phillipi/pix2pix
[18]: http://ml4a.github.io
[17]: https://twitter.com/elluba
[16]: http://xcw.org.uk
[15]: http://photo-school.co.uk/walks
[14]: https://vimeo.com/201191882
[13]: http://feltron.com/PhotoViz.html
[12]: http://www.gold.ac.uk/pg/ma-photography-urban-cultures/
[11]: https://en.wikipedia.org/wiki/Wardriving
[10]: http://hfwas.co.uk/?page_id=27
[9]: https://emily-warner.com
[8]: https://medium.com/@kcimc/a-return-to-machine-learning-2de3728558eb
[1]: http://ml4a.github.io
[2]: http://frnsys.com
[3]: http://www.fizzpop.org.uk
[6]: http://www.pcafe.co.uk 
[4]: http://www.vividprojects.org.uk/programme/black-hole-club-2017/
[5]: http://www.bom.org.uk
[7]: http://resonate.io/2017/